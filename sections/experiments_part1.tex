% Experiments Section - Part 1: Setup and Main Results
\section{Experiments}
\label{sec:experiments}

We evaluate DAA on four diverse real-world benchmarks using actual LLM inference on NVIDIA H100 GPUs, moving beyond simulation to demonstrate practical effectiveness. Our evaluation measures both task performance and computational cost, quantifying DAA's ability to maintain accuracy while reducing inference expenses through intelligent model selection.

\subsection{Experimental Setup}

\paragraph{Model Infrastructure.}

All experiments employ the Qwen2.5 model family~\cite{qwen2024qwen25} served via vLLM~\cite{kwon2023vllm} on NVIDIA H100 80GB GPUs. We deploy a heterogeneous model pool spanning four capacity tiers:

\begin{itemize}
\item \textbf{Economy} (1.5B): Qwen2.5-1.5B-Instruct --- lowest cost (\$0.10/1M tokens), suitable for simple pattern matching and formatting tasks
\item \textbf{Balanced} (3B): Qwen2.5-3B-Instruct --- moderate cost (\$0.20/1M tokens), capable of basic reasoning and extraction
\item \textbf{Premium} (7B): Qwen2.5-7B-Instruct --- standard cost (\$0.50/1M tokens), strong general-purpose reasoning
\item \textbf{Expert} (14B): Qwen2.5-14B-Instruct --- highest cost (\$1.00/1M tokens), specialized knowledge and complex multi-step reasoning
\end{itemize}

Each model is served as an OpenAI-compatible API endpoint with temperature $T=0$ (greedy decoding) for reproducibility. Model selection per benchmark is determined by DAA's Evidence-Aware Selective Escalation mechanism (\S\ref{sec:escalation}).

\paragraph{Benchmarks.}

We evaluate on four benchmarks spanning diverse task types:

\begin{enumerate}
\item \textbf{PlanCraft}~\cite{gautierdag2024plancraft}: Multi-step Minecraft crafting planning requiring domain-specific recipe knowledge. We use val.small.easy (50 tasks). \emph{Model pair}: 7B + 14B (domain knowledge requires larger models).

\item \textbf{WorkBench}~\cite{liu2024workbench}: Office automation across 6 domains (Analytics, Calendar, CRM, Email, Project Management, Multi-domain). 60 tasks total (10 per domain). \emph{Model pair}: 1.5B/3B + 7B.

\item \textbf{FinanceBench}~\cite{islam2024financebench}: Financial question answering with graded scoring across difficulty levels (Easy, Medium, Hard, Multi-step). 18 tasks. \emph{Model pair}: 1.5B/3B + 7B.

\item \textbf{BrowseComp-Plus}: Evidence-based factual question answering requiring extraction from provided documents. 30 tasks. \emph{Model pair}: 3B + 7B.
\end{enumerate}

\paragraph{Baselines.}

For each benchmark, we compare three strategies:
\begin{itemize}
\item \textbf{Large-Model Baseline}: All tasks processed by the largest available model (7B or 14B depending on benchmark)
\item \textbf{Small-Model Fixed}: All tasks processed by a smaller fixed model (3B or 7B)
\item \textbf{DAA (Ours)}: Evidence-Aware Selective Escalation with benchmark-adaptive model pairing
\end{itemize}

\paragraph{Evaluation Metrics.}

\begin{itemize}
\item \textbf{Task Success Rate} (\%): Fraction of correctly completed tasks (PlanCraft, WorkBench, BrowseComp+)
\item \textbf{Average Score}: Mean quality score across tasks (FinanceBench, graded 0--1)
\item \textbf{Relative Cost} (\%): Inference cost as percentage of large-model baseline
\item \textbf{Cost Savings} (\%): Reduction in inference cost vs baseline
\item \textbf{Cost-Efficiency Ratio}: Success rate divided by relative cost (higher is better)
\end{itemize}

\subsection{Evidence-Aware Selective Escalation}
\label{sec:escalation}

Based on empirical analysis of model behavior across benchmarks, we develop a three-way response classification that forms the core of DAA's practical model selection:

\begin{enumerate}
\item \textbf{Confident}: The cheap model produces a specific, well-formed answer with high lexical confidence $\rightarrow$ \emph{accept cheap model's answer} (no escalation needed)
\item \textbf{Hard Unknown}: The cheap model quickly indicates inability to answer (e.g., ``not provided,'' ``unknown'') with short response length $\rightarrow$ \emph{skip escalation} (expensive model would also fail)
\item \textbf{Soft Uncertain}: The cheap model attempts reasoning but cannot reach a conclusion, producing lengthy uncertain responses $\rightarrow$ \emph{escalate to expensive model} (additional capacity may help)
\end{enumerate}

This classification reduces wasteful escalation by $79\%$ compared to naive ``always escalate when uncertain'' strategies. The key insight is distinguishing between \emph{model capacity limitations} (soft uncertain, where a larger model helps) and \emph{information limitations} (hard unknown, where no model can succeed).

\subsection{Main Results}
\label{sec:main_results}

Table~\ref{tab:main_results} presents our primary experimental findings across all four benchmarks.

\begin{table}[t]
\centering
\caption{Cross-benchmark DAA evaluation results. DAA achieves equal or superior performance to large-model baselines while reducing costs by 8--66\%. Efficiency ratio measures performance per unit cost (higher is better). Best results per benchmark in \textbf{bold}.}
\label{tab:main_results}
\begin{tabular}{llcccc}
\toprule
\textbf{Benchmark} & \textbf{Method} & \textbf{Metric} & \textbf{Cost} & \textbf{Savings} & \textbf{Efficiency} \\
\midrule
\multirow{3}{*}{PlanCraft}
 & 14B Baseline & 64.0\% & 100\% & --- & 0.640 \\
 & 7B Fixed     & 42.0\% & 50\%  & 50\% & 0.840 \\
 & \textbf{DAA v5} & \textbf{70.0\%} & 92\% & \textbf{8\%} & \textbf{0.761} \\
\midrule
\multirow{3}{*}{WorkBench}
 & 7B Baseline  & 10.0\% & 100\% & --- & 0.100 \\
 & 3B Fixed     & 6.7\%  & 40\%  & 60\% & 0.167 \\
 & \textbf{DAA}  & \textbf{10.0\%} & 34.3\% & \textbf{65.7\%} & \textbf{0.291} \\
\midrule
\multirow{3}{*}{FinanceBench}
 & 7B Baseline  & 0.859  & 100\% & --- & 0.859 \\
 & 3B Fixed     & 0.825  & 40\%  & 60\% & 2.062 \\
 & \textbf{DAA}  & \textbf{0.859} & 67.8\% & \textbf{32.2\%} & \textbf{1.268} \\
\midrule
\multirow{3}{*}{BrowseComp+}
 & 7B Baseline  & 40.0\% & 100\% & --- & 0.400 \\
 & 3B Fixed     & 36.7\% & 40\%  & 60\% & 0.917 \\
 & \textbf{DAA v5} & \textbf{40.0\%} & 53.3\% & \textbf{46.7\%} & \textbf{0.750} \\
\midrule
\multicolumn{2}{l}{\textbf{Cross-Benchmark Average}} & $\geq$ Baseline & --- & \textbf{39.0\%} & \textbf{1.9$\times$} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Finding 1: DAA Matches or Exceeds Baselines Across All Benchmarks.}

On PlanCraft, DAA achieves \textbf{70.0\%} success rate, exceeding the 14B baseline (64.0\%) by +6 percentage points (+9.4\% relative improvement). On WorkBench, FinanceBench, and BrowseComp+, DAA exactly matches the large-model baseline while operating at substantially lower cost. Crucially, DAA \emph{never} degrades performance below the baseline on any benchmark.

\paragraph{Key Finding 2: Average 39\% Cost Reduction.}

Across all four benchmarks, DAA reduces inference costs by \textbf{39.0\%} on average, ranging from 8.0\% (PlanCraft, where domain-specific knowledge necessitates frequent 14B usage) to 65.7\% (WorkBench, where most tasks are solvable by 1.5B models). This translates to \textbf{1.9$\times$ average efficiency improvement}.

\paragraph{Key Finding 3: Best-of-Both-Models Effect (PlanCraft).}

PlanCraft reveals a remarkable phenomenon: DAA \emph{exceeds} both individual model baselines by combining their complementary strengths:
\begin{itemize}
\item 7B produces 100\% accurate \texttt{[craft]} responses (21/21) for recipes it knows
\item 14B covers recipes that 7B cannot solve, correctly answering 66.7\% (14/21) of escalated tasks
\item DAA selectively combines: use 7B's confident answers + 14B's broader knowledge = \textbf{Best-of-Both}
\end{itemize}

This effect demonstrates that intelligent orchestration can exceed any single model's capability, a result not achievable by fixed model deployment.

\subsection{Per-Benchmark Analysis}

\paragraph{PlanCraft: Domain Knowledge Escalation (7B $\rightarrow$ 14B).}

DAA's three-way decision breakdown on PlanCraft:
\begin{itemize}
\item \textbf{7B Confident} (\texttt{[craft]}): 21 tasks (42\%) --- accuracy 100\% (21/21), cost: 7B only
\item \textbf{Skipped} (hopeless): 8 tasks (16\%) --- accuracy 0\% (both models fail), cost: 7B only (avoids wasting 14B)
\item \textbf{Escalated to 14B}: 21 tasks (42\%) --- accuracy 66.7\% (14/21), cost: 7B + 14B
\end{itemize}

The ``skipped'' category is critical: these 8 tasks involve items requiring smelting or unavailable ingredients where even 14B fails. By identifying and skipping them, DAA saves 16\% of potential escalation costs without any accuracy loss.

\paragraph{WorkBench: Extreme Cost Savings via Domain Routing.}

WorkBench exhibits the largest cost savings (65.7\%) because DAA correctly identifies that 5 of 6 domains (Analytics, Calendar, CRM, Email, Project Management) are solvable by the 1.5B model, reserving 7B only for Multi-domain tasks requiring cross-system reasoning:

\begin{itemize}
\item 1.5B (Economy): 47 calls (78\%) --- handles simple single-domain tasks
\item 3B (Balanced): 3 calls (5\%) --- intermediate complexity
\item 7B (Premium): 10 calls (17\%) --- multi-domain tasks only
\end{itemize}

Notably, on Project Management tasks, DAA achieves 60\% success rate using only the 1.5B model --- \emph{identical} to the 7B baseline but at 1/5th the cost.

\paragraph{FinanceBench: Perfect Difficulty-Adaptive Selection.}

FinanceBench demonstrates DAA's ability to match model capacity to task difficulty:
\begin{itemize}
\item \textbf{Easy} tasks: 1.5B selected (score 0.790, vs 7B's 0.830 --- 95\% quality at 20\% cost)
\item \textbf{Medium} tasks: 3B/7B mixture (score 0.833, \emph{exceeding} 7B's 0.793)
\item \textbf{Hard} tasks: 7B selected (score 0.890, matching 7B baseline)
\item \textbf{Multi-step} tasks: 7B selected (score 0.967, matching 7B baseline)
\end{itemize}

On Medium-difficulty tasks, the 3B model actually \emph{outperforms} 7B (0.833 vs 0.793), suggesting that smaller models may provide more focused responses for moderately complex financial questions. DAA captures this by routing Medium tasks to the appropriate model tier.

\paragraph{BrowseComp-Plus: Refined Selective Escalation.}

BrowseComp+ showcases the evolution of DAA's escalation strategy through iterative refinement (v1 through v5):
\begin{itemize}
\item \textbf{v1}: 40.0\% accuracy, 0\% savings (all queries sent to 7B)
\item \textbf{v3}: 40.0\% accuracy, 10\% savings (escalate all uncertain --- 93\% wasteful)
\item \textbf{v4}: 36.7\% accuracy, 60\% savings (never escalate --- lost 1 correct answer)
\item \textbf{v5}: \textbf{40.0\% accuracy, 46.7\% savings} (selective escalation --- optimal)
\end{itemize}

The v5 three-way classification achieves the critical balance:
\begin{itemize}
\item \textbf{Confident}: 16 tasks (53\%) --- 3B accuracy 68.8\% (11/16)
\item \textbf{Hard Unknown}: 10 tasks (33\%) --- 0\% for both models (7B escalation would be entirely wasted)
\item \textbf{Soft Uncertain}: 4 tasks (13\%) --- escalation recovers 1 additional correct answer
\end{itemize}

This reduces wasteful escalation from 14 calls (v3) to just 3 calls (v5), a \textbf{79\% reduction} in unnecessary expensive model invocations while maintaining identical accuracy.
