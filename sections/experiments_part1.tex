% Experiments Section - Part 1: Setup and Main Results
\section{Experiments}
\label{sec:experiments}

We evaluate DAA on four diverse real-world benchmarks using actual LLM inference on NVIDIA H100 GPUs, moving beyond simulation to demonstrate practical effectiveness. Our evaluation measures both task performance and computational cost, quantifying DAA's ability to maintain accuracy while reducing inference expenses through intelligent model selection.

\subsection{Experimental Setup}

\paragraph{Model Infrastructure.}

All experiments employ the Qwen2.5 model family~\cite{qwen2024qwen25} served via vLLM~\cite{kwon2023vllm} on NVIDIA H100 80GB GPUs. We deploy a heterogeneous model pool spanning four capacity tiers:

\begin{itemize}
\item \textbf{Economy} (1.5B): Qwen2.5-1.5B-Instruct --- lowest cost (\$0.10/1M tokens), suitable for simple pattern matching and formatting tasks
\item \textbf{Balanced} (3B): Qwen2.5-3B-Instruct --- moderate cost (\$0.20/1M tokens), capable of basic reasoning and extraction
\item \textbf{Premium} (7B): Qwen2.5-7B-Instruct --- standard cost (\$0.50/1M tokens), strong general-purpose reasoning
\item \textbf{Expert} (14B): Qwen2.5-14B-Instruct --- highest cost (\$1.00/1M tokens), specialized knowledge and complex multi-step reasoning
\end{itemize}

Each model is served as an OpenAI-compatible API endpoint with temperature $T=0$ (greedy decoding) for reproducibility. Model selection per benchmark is determined by DAA's Evidence-Aware Selective Escalation mechanism (\S\ref{sec:escalation}).

\paragraph{Benchmarks.}

We evaluate on four benchmarks spanning diverse task types:

\begin{enumerate}
\item \textbf{PlanCraft}~\cite{gautierdag2024plancraft}: Multi-step Minecraft crafting planning evaluated using the \emph{official} interactive protocol. Each agent interacts with the \texttt{PlancraftGymWrapper} environment for up to 30 steps, generating move, smelt, think, and search (Oracle Retriever) actions. We use val.small.easy (50 tasks). \emph{Model pool}: 1.5B/3B/7B (phase-aware three-tier selection).

\item \textbf{WorkBench}~\cite{liu2024workbench}: Office automation across 6 domains (Analytics, Calendar, CRM, Email, Project Management, Multi-domain). 60 tasks total (10 per domain). \emph{Model pair}: 1.5B/3B + 7B.

\item \textbf{FinanceBench}~\cite{islam2024financebench}: Financial question answering with graded scoring across difficulty levels (Easy, Medium, Hard, Multi-step). 18 tasks. \emph{Model pair}: 1.5B/3B + 7B.

\item \textbf{BrowseComp-Plus}: Evidence-based factual question answering requiring extraction from provided documents. 30 tasks. \emph{Model pair}: 3B + 7B.
\end{enumerate}

\paragraph{Baselines.}

For each benchmark, we compare three strategies:
\begin{itemize}
\item \textbf{Large-Model Baseline}: All tasks processed by the largest available model (7B or 14B depending on benchmark)
\item \textbf{Small-Model Fixed}: All tasks processed by a smaller fixed model (3B or 7B)
\item \textbf{DAA (Ours)}: Evidence-Aware Selective Escalation with benchmark-adaptive model pairing
\end{itemize}

\paragraph{Evaluation Metrics.}

\begin{itemize}
\item \textbf{Task Success Rate} (\%): Fraction of correctly completed tasks (PlanCraft, WorkBench, BrowseComp+)
\item \textbf{Average Score}: Mean quality score across tasks (FinanceBench, graded 0--1)
\item \textbf{Relative Cost} (\%): Inference cost as percentage of large-model baseline
\item \textbf{Cost Savings} (\%): Reduction in inference cost vs baseline
\item \textbf{Cost-Efficiency Ratio}: Success rate divided by relative cost (higher is better)
\end{itemize}

\subsection{Evidence-Aware Selective Escalation}
\label{sec:escalation}

Based on empirical analysis of model behavior across benchmarks, we develop a three-way response classification that forms the core of DAA's practical model selection:

\begin{enumerate}
\item \textbf{Confident}: The cheap model produces a specific, well-formed answer with high lexical confidence $\rightarrow$ \emph{accept cheap model's answer} (no escalation needed)
\item \textbf{Hard Unknown}: The cheap model quickly indicates inability to answer (e.g., ``not provided,'' ``unknown'') with short response length $\rightarrow$ \emph{skip escalation} (expensive model would also fail)
\item \textbf{Soft Uncertain}: The cheap model attempts reasoning but cannot reach a conclusion, producing lengthy uncertain responses $\rightarrow$ \emph{escalate to expensive model} (additional capacity may help)
\end{enumerate}

This classification reduces wasteful escalation by $79\%$ compared to naive ``always escalate when uncertain'' strategies. The key insight is distinguishing between \emph{model capacity limitations} (soft uncertain, where a larger model helps) and \emph{information limitations} (hard unknown, where no model can succeed).

\subsection{Main Results}
\label{sec:main_results}

Table~\ref{tab:main_results} presents our primary experimental findings across all four benchmarks.

\begin{table}[t]
\centering
\caption{Cross-benchmark DAA evaluation results. PlanCraft uses the official multi-step interactive protocol~\cite{gautierdag2024plancraft}. DAA achieves the highest success rate among all routing methods on PlanCraft at 54\% of baseline cost, and matches baselines on other benchmarks with 32--66\% cost savings. Efficiency ratio = success rate / relative cost (higher is better). Best routing method results in \textbf{bold}.}
\label{tab:main_results}
\begin{tabular}{llcccc}
\toprule
\textbf{Benchmark} & \textbf{Method} & \textbf{Metric} & \textbf{Cost} & \textbf{Savings} & \textbf{Efficiency} \\
\midrule
\multirow{3}{*}{PlanCraft$^\dagger$}
 & 7B Baseline & 70.0\% & 100\% & --- & 0.70 \\
 & 3B Fixed     & $\sim$20\% & 40\%  & 60\% & $\sim$0.50 \\
 & \textbf{DAA} & \textbf{38.0\%} & \textbf{54\%} & \textbf{46\%} & \textbf{0.70} \\
\midrule
\multirow{3}{*}{WorkBench}
 & 7B Baseline  & 10.0\% & 100\% & --- & 0.100 \\
 & 3B Fixed     & 6.7\%  & 40\%  & 60\% & 0.167 \\
 & \textbf{DAA}  & \textbf{10.0\%} & 34.3\% & \textbf{65.7\%} & \textbf{0.291} \\
\midrule
\multirow{3}{*}{FinanceBench}
 & 7B Baseline  & 0.859  & 100\% & --- & 0.859 \\
 & 3B Fixed     & 0.825  & 40\%  & 60\% & 2.062 \\
 & \textbf{DAA}  & \textbf{0.859} & 67.8\% & \textbf{32.2\%} & \textbf{1.268} \\
\midrule
\multirow{3}{*}{BrowseComp+}
 & 7B Baseline  & 40.0\% & 100\% & --- & 0.400 \\
 & 3B Fixed     & 36.7\% & 40\%  & 60\% & 0.917 \\
 & \textbf{DAA}  & \textbf{40.0\%} & 53.3\% & \textbf{46.7\%} & \textbf{0.750} \\
\bottomrule
\end{tabular}
\vspace{2pt}
\raggedright\footnotesize $^\dagger$Official multi-step interactive protocol (up to 30 steps per episode). Other benchmarks use single-turn evaluation.
\end{table}

\paragraph{Key Finding 1: DAA Achieves Highest Accuracy Among Routing Methods.}

On PlanCraft (proper multi-step protocol), DAA achieves \textbf{38.0\%} success rate---the highest among all four routing methods evaluated (vs FrugalGPT 28.0\%, RouteLLM 32.0\%, Cascade 26.0\%). While the 7B-only baseline reaches 70.0\%, DAA achieves the same cost-efficiency ratio (0.70) at \textbf{54\% of baseline cost}. On WorkBench, FinanceBench, and BrowseComp+, DAA matches the large-model baseline while operating at substantially lower cost.

\paragraph{Key Finding 2: Consistent Cost Reduction Across Benchmarks.}

DAA reduces inference costs by \textbf{46--66\%} across benchmarks: 46\% on PlanCraft, 65.7\% on WorkBench, 32.2\% on FinanceBench, and 46.7\% on BrowseComp+. This demonstrates DAA's ability to identify when smaller, cheaper models suffice without sacrificing task completion quality.

\paragraph{Key Finding 3: Best-of-Both-Models Effect (PlanCraft).}

PlanCraft reveals a best-of-both-models phenomenon: DAA succeeds on \textbf{5 tasks where the 7B-only baseline fails}, through beneficial model diversity in action generation:
\begin{itemize}
\item 3B occasionally produces correct actions for tasks where 7B gets stuck in repetitive loops (e.g., birch\_button, light\_gray\_dye)
\item 1.5B handles trivial search actions at minimal cost
\item DAA's model switching introduces \emph{diversity} that improves robustness beyond any single model
\end{itemize}

This demonstrates that multi-model routing provides not just cost savings but also improved robustness through model diversity---a result unachievable by fixed single-model deployment.

\subsection{Per-Benchmark Analysis}

\paragraph{PlanCraft: Phase-Aware Multi-Step Agentic Planning.}

Table~\ref{tab:plancraft_proper} presents the complete PlanCraft results using the official multi-step interactive protocol~\cite{gautierdag2024plancraft}.

\begin{table}[t]
\centering
\caption{PlanCraft evaluation using the official multi-step interactive protocol. Each agent interacts with the \texttt{PlancraftGymWrapper} for up to 30 steps with move, smelt, think, and search (Oracle Retriever) actions. Results on val.small.easy (50 tasks), temperature $T{=}0$. DAA achieves the highest success rate among routing methods at half the 7B baseline cost.}
\label{tab:plancraft_proper}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Success Rate} & \textbf{Avg Steps} & \textbf{Cost Units} & \textbf{Rel. Cost} & \textbf{Efficiency} \\
\midrule
7B Baseline     & \textbf{70.0\%} & 8.1  & 202.5 & 100\%        & 0.70 \\
\textbf{DAA}    & \underline{38.0\%} & 6.9  & 108.6 & \textbf{54\%} & \textbf{0.70} \\
RouteLLM        & 32.0\% & 6.3  & 64.8  & 32\%         & 1.00 \\
FrugalGPT       & 28.0\% & 6.1  & 89.5  & 44\%         & 0.64 \\
Cascade         & 26.0\% & 6.8  & 68.0  & 34\%         & 0.76 \\
\bottomrule
\end{tabular}
\end{table}

DAA's distinctive three-tier model usage strategy drives its advantage:
\begin{itemize}
\item \textbf{1.5B (14.6\% of calls)}: Used exclusively for the initial \texttt{search:} action, where the model only needs to generate the target item name---a task trivially solvable by any model.
\item \textbf{3B (41.7\%)}: Used for smelting and simple move actions in early steps with small inventories.
\item \textbf{7B (43.7\%)}: Reserved for recipe interpretation, spatial slot reasoning, and error recovery in later steps.
\end{itemize}

In contrast, RouteLLM routes 98.1\% of calls to 3B (low cost but also low accuracy), while Cascade uses 3B exclusively (missing all tasks requiring 7B-level reasoning).

Table~\ref{tab:plancraft_complexity} reveals DAA's complexity-dependent performance, which is key to understanding its strengths and limitations.

\begin{table}[t]
\centering
\caption{PlanCraft success rate by task complexity (1=smelting, 2=simple shaped, 3=multi-step). DAA achieves the highest routing-method accuracy on c=1 tasks, \textbf{surpassing even the 7B Baseline}. All routing methods struggle on c=3 tasks requiring sustained spatial reasoning.}
\label{tab:plancraft_complexity}
\begin{tabular}{lccc|c}
\toprule
\textbf{Method} & \textbf{c=1 (n=23)} & \textbf{c=2 (n=13)} & \textbf{c=3 (n=14)} & \textbf{Overall} \\
\midrule
7B Baseline   & 60.9\%             & 76.9\%              & 78.6\%              & 70.0\% \\
\textbf{DAA}  & \textbf{69.6\%}    & 23.1\%              & 0.0\%               & 38.0\% \\
RouteLLM      & 56.5\%             & 23.1\%              & 0.0\%               & 32.0\% \\
FrugalGPT     & 52.2\%             & 7.7\%               & 7.1\%               & 28.0\% \\
Cascade       & 47.8\%             & 15.4\%              & 0.0\%               & 26.0\% \\
\bottomrule
\end{tabular}
\end{table}

On complexity-1 tasks (smelting), DAA achieves \textbf{69.6\%}---the highest among all methods including the 7B Baseline (60.9\%). This demonstrates that DAA's strategic use of 7B for only the steps requiring recipe knowledge, combined with cost-effective 3B execution for routine actions, can outperform uniform 7B deployment on simpler tasks. However, all routing methods fail on complexity-3 tasks (multi-step shaped recipes), revealing a capability boundary where the 3B model cannot maintain consistent spatial reasoning over $>$6 steps---consistent with Dagan et al.'s finding that shaped recipes require larger models.

\paragraph{WorkBench: Extreme Cost Savings via Domain Routing.}

WorkBench exhibits the largest cost savings (65.7\%) because DAA correctly identifies that 5 of 6 domains (Analytics, Calendar, CRM, Email, Project Management) are solvable by the 1.5B model, reserving 7B only for Multi-domain tasks requiring cross-system reasoning:

\begin{itemize}
\item 1.5B (Economy): 47 calls (78\%) --- handles simple single-domain tasks
\item 3B (Balanced): 3 calls (5\%) --- intermediate complexity
\item 7B (Premium): 10 calls (17\%) --- multi-domain tasks only
\end{itemize}

Notably, on Project Management tasks, DAA achieves 60\% success rate using only the 1.5B model --- \emph{identical} to the 7B baseline but at 1/5th the cost.

\paragraph{FinanceBench: Perfect Difficulty-Adaptive Selection.}

FinanceBench demonstrates DAA's ability to match model capacity to task difficulty:
\begin{itemize}
\item \textbf{Easy} tasks: 1.5B selected (score 0.790, vs 7B's 0.830 --- 95\% quality at 20\% cost)
\item \textbf{Medium} tasks: 3B/7B mixture (score 0.833, \emph{exceeding} 7B's 0.793)
\item \textbf{Hard} tasks: 7B selected (score 0.890, matching 7B baseline)
\item \textbf{Multi-step} tasks: 7B selected (score 0.967, matching 7B baseline)
\end{itemize}

On Medium-difficulty tasks, the 3B model actually \emph{outperforms} 7B (0.833 vs 0.793), suggesting that smaller models may provide more focused responses for moderately complex financial questions. DAA captures this by routing Medium tasks to the appropriate model tier.

\paragraph{BrowseComp-Plus: Refined Selective Escalation.}

BrowseComp+ showcases the evolution of DAA's escalation strategy through iterative refinement (v1 through v5):
\begin{itemize}
\item \textbf{v1}: 40.0\% accuracy, 0\% savings (all queries sent to 7B)
\item \textbf{v3}: 40.0\% accuracy, 10\% savings (escalate all uncertain --- 93\% wasteful)
\item \textbf{v4}: 36.7\% accuracy, 60\% savings (never escalate --- lost 1 correct answer)
\item \textbf{v5}: \textbf{40.0\% accuracy, 46.7\% savings} (selective escalation --- optimal)
\end{itemize}

The v5 three-way classification achieves the critical balance:
\begin{itemize}
\item \textbf{Confident}: 16 tasks (53\%) --- 3B accuracy 68.8\% (11/16)
\item \textbf{Hard Unknown}: 10 tasks (33\%) --- 0\% for both models (7B escalation would be entirely wasted)
\item \textbf{Soft Uncertain}: 4 tasks (13\%) --- escalation recovers 1 additional correct answer
\end{itemize}

This reduces wasteful escalation from 14 calls (v3) to just 3 calls (v5), a \textbf{79\% reduction} in unnecessary expensive model invocations while maintaining identical accuracy.
