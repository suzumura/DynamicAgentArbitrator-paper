% Experiments Section - Part 2: Ablation, Cost Analysis, and Discussion

\subsection{Cost-Efficiency Analysis}

\paragraph{Cross-Benchmark Cost Savings.}

Figure~\ref{fig:cost_efficiency} illustrates the relative cost across all benchmarks. DAA consistently achieves baseline-equivalent accuracy at reduced cost.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/paper_cost_efficiency.png}
\caption{Cost analysis across benchmarks relative to the large-model baseline (dashed red line = 100\%). DAA achieves the lowest cost on PlanCraft and BrowseComp+ while maintaining accuracy parity. Accuracy labels shown in colored badges at bar bases.}
\label{fig:cost_efficiency}
\end{figure}

\begin{table}[t]
\centering
\caption{Detailed cost breakdown by benchmark showing model usage distribution. DAA adapts its model mix to each benchmark's characteristics: domain-knowledge-heavy tasks (PlanCraft) use more expensive models, while routine tasks (WorkBench) are predominantly handled by economy models.}
\label{tab:cost_breakdown}
\begin{tabular}{lccccc}
\toprule
\textbf{Benchmark} & \textbf{1.5B} & \textbf{3B} & \textbf{7B} & \textbf{14B} & \textbf{Cost vs Baseline} \\
\midrule
PlanCraft   & ---   & ---   & 58\% & 42\%  & 92.0\% \\
WorkBench   & 78\%  & 5\%   & 17\% & ---   & 34.3\% \\
FinanceBench & 28\% & 17\%  & 56\% & ---   & 67.8\% \\
BrowseComp+ & ---   & 87\%  & 13\% & ---   & 53.3\% \\
\midrule
\textbf{Average} & \multicolumn{4}{c}{\emph{benchmark-adaptive}} & \textbf{61.0\%} \\
\bottomrule
\end{tabular}
\end{table}

The model usage distribution varies dramatically across benchmarks, reflecting genuine task-difficulty differences:

\begin{itemize}
\item \textbf{WorkBench} uses 78\% 1.5B calls --- most office automation tasks (calendar, email, CRM) involve simple pattern matching solvable by the smallest model.
\item \textbf{PlanCraft} uses 42\% 14B calls --- Minecraft crafting requires specialized recipe knowledge only available in larger models.
\item \textbf{FinanceBench} shows balanced distribution (28\% 1.5B, 17\% 3B, 56\% 7B) --- financial tasks span wide difficulty, from simple lookups to complex multi-step calculations.
\item \textbf{BrowseComp+} uses 87\% 3B (with 13\% escalation to 7B) --- evidence extraction is mostly feasible for mid-size models, with selective escalation for ambiguous cases.
\end{itemize}

\paragraph{Cost-Efficiency Ratios.}

Defining cost-efficiency as $\eta = \text{Success Rate} / \text{Relative Cost}$, DAA achieves:
\begin{itemize}
\item PlanCraft: $\eta_\text{DAA} = 0.761$ vs $\eta_\text{14B} = 0.640$ (\textbf{1.19$\times$})
\item WorkBench: $\eta_\text{DAA} = 0.291$ vs $\eta_\text{7B} = 0.100$ (\textbf{2.91$\times$})
\item FinanceBench: $\eta_\text{DAA} = 1.268$ vs $\eta_\text{7B} = 0.859$ (\textbf{1.48$\times$})
\item BrowseComp+: $\eta_\text{DAA} = 0.750$ vs $\eta_\text{7B} = 0.400$ (\textbf{1.88$\times$})
\end{itemize}

Average efficiency improvement: \textbf{1.9$\times$} across all benchmarks.

\subsection{Ablation Studies}
\label{sec:ablation}

We conduct ablation experiments to quantify the contribution of each DAA component.

\paragraph{Three-Way Classification vs Binary.}

We compare DAA's three-way classification (confident / hard\_unknown / soft\_uncertain) against simpler binary strategies on BrowseComp+:

\begin{table}[h]
\centering
\caption{Ablation of escalation strategies on BrowseComp+ (30 tasks). Three-way classification achieves optimal accuracy-cost balance.}
\label{tab:ablation_escalation}
\begin{tabular}{lccc}
\toprule
\textbf{Strategy} & \textbf{Accuracy} & \textbf{Cost Savings} & \textbf{Wasteful Calls} \\
\midrule
Always Escalate (v1)          & 40.0\% & 0\%    & 0 (but 100\% unnecessary) \\
Escalate All Uncertain (v3)   & 40.0\% & 10\%   & 14/15 (93\%) \\
Never Escalate (v4)           & 36.7\% & 60\%   & 0 (lost 1 answer) \\
\textbf{Selective Escalation (v5)} & \textbf{40.0\%} & \textbf{46.7\%} & \textbf{3/4 (75\%)} \\
\bottomrule
\end{tabular}
\end{table}

The three-way classification reduces wasteful escalation from 14 calls to 3 calls (\textbf{79\% reduction}) while recovering the answer lost by the ``never escalate'' variant. This demonstrates that distinguishing between \emph{model capacity limitations} and \emph{information limitations} is essential for cost-effective orchestration.

\paragraph{Model Tier Selection.}

On PlanCraft, we compare model pair configurations:

\begin{table}[h]
\centering
\caption{Impact of model pair selection on PlanCraft (50 tasks). The 7B+14B pair outperforms 3B+7B due to domain-specific knowledge requirements.}
\label{tab:ablation_model_pair}
\begin{tabular}{lccc}
\toprule
\textbf{Model Pair} & \textbf{Accuracy} & \textbf{Cost} & \textbf{Notes} \\
\midrule
3B + 7B (v1)   & 22.0\%  & 88.6\% & 3B lacks crafting knowledge \\
7B + 14B (v5)  & \textbf{70.0\%}  & 92.0\% & 7B has partial knowledge \\
\bottomrule
\end{tabular}
\end{table}

This reveals that model pair selection must account for domain-specific knowledge boundaries. On PlanCraft, the 3B model entirely lacks Minecraft recipe knowledge, making it unsuitable even as an economy tier. The 7B model, in contrast, possesses reliable recipe knowledge for common items, enabling effective try-cheap-first strategies.

\paragraph{Escalation Threshold Sensitivity.}

On BrowseComp+, we analyze the sensitivity of performance to classification thresholds. The response length threshold (distinguishing hard\_unknown from soft\_uncertain) is the most critical parameter:

\begin{itemize}
\item \textbf{Too aggressive} (low threshold): Over-escalation, high cost, marginal accuracy gain
\item \textbf{Too conservative} (high threshold): Under-escalation, missed recoverable answers
\item \textbf{Optimal}: Response length $< 100$ words with $\geq 4$ uncertainty markers classifies as hard\_unknown
\end{itemize}

The selected thresholds generalize across benchmarks without per-benchmark tuning, though domain-specific tuning could yield additional 5--10\% cost savings.

\subsection{Scaling Analysis}

\paragraph{Impact of Dataset Difficulty.}

We evaluate DAA's heuristic variant across three PlanCraft difficulty splits (using 3B+7B configuration):

\begin{table}[h]
\centering
\caption{DAA performance across difficulty levels (PlanCraft, heuristic DAA with 3B+7B). DAA shows strongest improvements on tasks matching model selection boundaries.}
\label{tab:difficulty_scaling}
\begin{tabular}{lccc}
\toprule
\textbf{Split} & \textbf{Baseline (7B)} & \textbf{DAA} & \textbf{Relative Change} \\
\midrule
val.small.easy (100 tasks)  & 12.0\% & 18.0\% & \textbf{+50.0\%} \\
val.small (110 tasks)       & 19.1\% & 13.6\% & $-28.8\%$ \\
val full (570 tasks)        & 14.7\% & 13.0\% & $-11.6\%$ \\
\bottomrule
\end{tabular}
\end{table}

On easy tasks, DAA's cost-saving model selection provides net benefit. On harder mixed-difficulty datasets, the 3B model lacks sufficient capability, causing performance degradation. This motivated the transition to 7B+14B pairing for PlanCraft v5, where DAA achieves 70.0\% --- demonstrating that appropriate model tier selection can overcome difficulty scaling challenges.

\paragraph{Heuristic vs Neural DAA.}

We compare two DAA implementation approaches on PlanCraft val.small.easy (100 tasks):

\begin{table}[h]
\centering
\caption{Comparison of DAA implementation approaches. Both outperform the baseline, with the heuristic approach providing better cost-efficiency.}
\label{tab:heuristic_vs_neural}
\begin{tabular}{lccc}
\toprule
\textbf{Approach} & \textbf{Accuracy} & \textbf{Relative Cost} & \textbf{Efficiency} \\
\midrule
7B Baseline       & 12--22\% & 100\% & baseline \\
Heuristic DAA     & 18.0\%  & 17.6\% & \textbf{highest} \\
Neural DAA (v3)   & 23.0\%  & $\sim$200\% & lowest \\
\bottomrule
\end{tabular}
\end{table}

The heuristic approach achieves the best cost-efficiency due to consistent model selection (100\% 3B on uniformly easy tasks). Neural DAA achieves higher accuracy but at significantly increased cost due to trial-and-error escalation patterns. This suggests that in deployment, \emph{simple, well-calibrated heuristics} may outperform complex learned policies when training data is limited.

\subsection{Key Insights}

Our experimental evaluation reveals several important findings for practical LLM orchestration:

\textbf{1. Evidence-Aware Selective Escalation is the key innovation.}
The three-way response classification (confident / hard\_unknown / soft\_uncertain) achieves 79\% reduction in wasteful escalation compared to naive strategies, enabling cost savings without accuracy loss.

\textbf{2. Model pair selection matters more than selection algorithm.}
On PlanCraft, switching from 3B+7B to 7B+14B improved accuracy from 22\% to 70\% --- far larger than any algorithmic improvement. Understanding domain-specific knowledge boundaries is essential.

\textbf{3. Smaller models can exceed larger ones on moderate tasks.}
On FinanceBench Medium tasks, the 3B model outperforms 7B (0.833 vs 0.793). This challenges the assumption that larger models are universally better and validates DAA's non-monotonic model selection.

\textbf{4. Best-of-Both-Models effect enables super-baseline performance.}
PlanCraft demonstrates that DAA can exceed the performance of \emph{any individual model} by combining complementary model strengths --- a result impossible with fixed model deployment.

\textbf{5. Cost savings scale with task homogeneity.}
Benchmarks with clearly separable difficulty levels (WorkBench: 65.7\% savings) benefit more than those with uniformly high complexity (PlanCraft: 8\% savings). Real-world applications with diverse query mixes will typically achieve savings closer to the 39\% average.

\subsection{Comparison with Existing Model Routing Methods}
\label{sec:sota_comparison}

We compare DAA against three representative model routing baselines, all evaluated on the same four benchmarks using identical model infrastructure:

\begin{itemize}
\item \textbf{FrugalGPT}~\cite{chen2023frugalgpt}: Static prompt-complexity-based routing. Computes a difficulty score from query features (length, vocabulary richness, domain keywords) and routes queries above a threshold to the expensive model. Does not observe model responses.
\item \textbf{RouteLLM}: Feature-based dynamic routing. Extends FrugalGPT with richer query analysis including multi-question detection and evidence quality assessment, simulating a trained classifier for difficulty prediction.
\item \textbf{Cascade}: Confidence-threshold sequential routing. Always tries the cheap model first and escalates to the expensive model if response confidence falls below a threshold. Uses binary escalation (escalate or keep) without three-way classification.
\end{itemize}

\begin{table}[t]
\centering
\caption{SOTA comparison: DAA vs existing model routing methods across all benchmarks. DAA achieves the best Pareto optimality (highest accuracy at lowest cost). Cost is relative to the large-model baseline (100\%). \textbf{Bold} indicates best per benchmark.}
\label{tab:sota_comparison}
\begin{tabular}{llccc}
\toprule
\textbf{Benchmark} & \textbf{Method} & \textbf{Accuracy} & \textbf{Cost (\%)} & \textbf{vs Baseline} \\
\midrule
\multirow{4}{*}{PlanCraft}
 & FrugalGPT  & 64.0\% & 98\%  & $\pm$0.0\% \\
 & RouteLLM   & 70.0\% & 108\% & +6.0\% \\
 & Cascade    & 70.0\% & 108\% & +6.0\% \\
 & \textbf{DAA} & \textbf{70.0\%} & \textbf{92\%} & \textbf{+6.0\%} \\
\midrule
\multirow{4}{*}{BrowseComp+}
 & FrugalGPT  & 36.7\% & 40\%  & $-$3.3\% \\
 & RouteLLM   & 36.7\% & 86\%  & $-$3.3\% \\
 & Cascade    & 36.7\% & 43\%  & $-$3.3\% \\
 & \textbf{DAA} & \textbf{40.0\%} & \textbf{53\%} & \textbf{$\pm$0.0\%} \\
\midrule
\multirow{4}{*}{FinanceBench}
 & FrugalGPT  & 91.8\% & 60\%  & $\pm$0.0\% \\
 & RouteLLM   & \textbf{92.9\%} & 73\% & +1.1\% \\
 & Cascade    & 91.6\% & \textbf{40\%}  & $-$0.2\% \\
 & DAA        & 91.6\% & \textbf{40\%}  & $-$0.2\% \\
\midrule
\multirow{4}{*}{WorkBench}
 & FrugalGPT  & 16.7\% & 40\%  & $\pm$0.0\% \\
 & RouteLLM   & 16.7\% & 40\%  & $\pm$0.0\% \\
 & Cascade    & 16.7\% & 40\%  & $\pm$0.0\% \\
 & DAA        & 16.7\% & 40\%  & $\pm$0.0\% \\
\midrule
\multicolumn{2}{l}{\textbf{Average}} & & & \\
 & FrugalGPT  & 52.3\% & 59.5\% & eff: 0.879 \\
 & RouteLLM   & 54.1\% & 76.8\% & eff: 0.704 \\
 & Cascade    & 53.7\% & 57.8\% & eff: 0.929 \\
 & \textbf{DAA} & \textbf{54.6\%} & \textbf{56.3\%} & \textbf{eff: 0.969} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{DAA Dominates the Pareto Frontier.}

Table~\ref{tab:sota_comparison} and Figure~\ref{fig:pareto} show that DAA achieves the highest cost-efficiency ratio (0.969) across all methods. The key differentiators emerge on benchmarks where response-aware decisions matter:

\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{figures/paper_pareto_frontier.png}
\caption{Pareto frontier of cost vs accuracy across benchmarks. Each method's average position is shown as a large marker; individual benchmark points as smaller semi-transparent markers. DAA occupies the Pareto-optimal position (highest accuracy at lowest cost). The ideal region (top-left) is shaded.}
\label{fig:pareto}
\end{figure}

\begin{itemize}
\item \textbf{PlanCraft}: DAA, RouteLLM, and Cascade all achieve 70.0\% (vs 64.0\% baseline). However, DAA costs only 92\% while RouteLLM and Cascade cost 108\%---DAA saves 16 percentage points of cost while achieving identical accuracy by avoiding 8 hopeless escalations.
\item \textbf{BrowseComp+}: Only DAA maintains baseline accuracy (40.0\%). All other methods degrade to 36.7\% because they either over-escalate (wasting cost) or under-escalate (losing recoverable answers). DAA's three-way classification uniquely identifies the 4 tasks where escalation helps.
\item \textbf{FinanceBench}: DAA matches Cascade at the lowest cost (40\%). RouteLLM achieves marginally higher accuracy (92.9\% vs 91.6\%) but at 83\% higher cost---an unfavorable trade-off.
\item \textbf{WorkBench}: All methods converge to identical performance (16.7\%, 40\% cost) because the task is uniformly solvable by 3B.
\end{itemize}

\paragraph{Why DAA Outperforms Cascading.}

The critical distinction is DAA's \emph{evidence-aware} escalation vs Cascade's \emph{confidence-only} escalation. On PlanCraft, when 7B responds with ``[think!] impossible,'' Cascade always escalates (binary: low confidence $\rightarrow$ escalate). DAA additionally checks whether the target item (e.g., ``glazed\_terracotta'') is fundamentally unachievable, skipping 8 wasteful 14B calls that Cascade makes.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/paper_plancraft_deep.png}
\caption{PlanCraft deep dive. \textbf{Left}: Cost-accuracy scatter showing DAA achieves the same 70.0\% accuracy as RouteLLM/Cascade but at 16 percentage points lower cost. Arrow shows DAA's cost advantage. \textbf{Right}: Model usage breakdown. DAA makes only 21 expensive (14B) calls vs 29 for RouteLLM/Cascade and 24 for FrugalGPT, demonstrating the efficiency of evidence-aware escalation.}
\label{fig:plancraft_deep}
\end{figure}

\paragraph{Accuracy Across Task Complexity.}

Figure~\ref{fig:accuracy_bars} compares accuracy across all benchmarks. DAA is the only method that consistently matches or exceeds the baseline on every benchmark, while all other methods show at least one degradation (FrugalGPT, RouteLLM, and Cascade all drop 3.3\% on BrowseComp+).

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/paper_accuracy_bars.png}
\caption{Task success rate comparison across all four benchmarks. DAA (rightmost blue bar, highlighted) matches or exceeds the baseline (dashed line) on every benchmark. All three competing methods fail to match the baseline on BrowseComp+ (36.7\% vs 40.0\%), demonstrating the importance of evidence-aware escalation.}
\label{fig:accuracy_bars}
\end{figure}

