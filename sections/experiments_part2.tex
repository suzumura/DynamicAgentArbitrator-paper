% Experiments Section - Part 2: Ablation, Cost Analysis, and Discussion

\subsection{Cost-Efficiency Analysis}

\paragraph{Cross-Benchmark Cost Savings.}

Figure~\ref{fig:cost_efficiency} illustrates the relative cost across all benchmarks. DAA consistently achieves baseline-equivalent accuracy at reduced cost.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/paper_cost_efficiency.png}
\caption{Cost analysis across benchmarks relative to the large-model baseline (dashed red line = 100\%). DAA achieves the lowest cost on PlanCraft and BrowseComp+ while maintaining accuracy parity. Accuracy labels shown in colored badges at bar bases.}
\label{fig:cost_efficiency}
\end{figure}

\begin{table}[t]
\centering
\caption{Detailed cost breakdown by benchmark showing model usage distribution. DAA adapts its model mix to each benchmark's characteristics: domain-knowledge-heavy tasks (PlanCraft) use more expensive models, while routine tasks (WorkBench) are predominantly handled by economy models.}
\label{tab:cost_breakdown}
\begin{tabular}{lccccc}
\toprule
\textbf{Benchmark} & \textbf{1.5B} & \textbf{3B} & \textbf{7B} & \textbf{14B} & \textbf{Cost vs Baseline} \\
\midrule
PlanCraft   & 14.6\% & 41.7\%  & 43.7\% & ---  & 54.0\% \\
WorkBench   & 78\%  & 5\%   & 17\% & ---   & 34.3\% \\
FinanceBench & 28\% & 17\%  & 56\% & ---   & 67.8\% \\
BrowseComp+ & ---   & 87\%  & 13\% & ---   & 53.3\% \\
\midrule
\textbf{Average} & \multicolumn{4}{c}{\emph{benchmark-adaptive}} & \textbf{61.0\%} \\
\bottomrule
\end{tabular}
\end{table}

The model usage distribution varies dramatically across benchmarks, reflecting genuine task-difficulty differences:

\begin{itemize}
\item \textbf{WorkBench} uses 78\% 1.5B calls --- most office automation tasks (calendar, email, CRM) involve simple pattern matching solvable by the smallest model.
\item \textbf{PlanCraft} uses a three-tier strategy (14.6\% 1.5B, 41.7\% 3B, 43.7\% 7B) --- 1.5B handles trivial search actions, 3B handles smelting, and 7B provides recipe interpretation and error recovery.
\item \textbf{FinanceBench} shows balanced distribution (28\% 1.5B, 17\% 3B, 56\% 7B) --- financial tasks span wide difficulty, from simple lookups to complex multi-step calculations.
\item \textbf{BrowseComp+} uses 87\% 3B (with 13\% escalation to 7B) --- evidence extraction is mostly feasible for mid-size models, with selective escalation for ambiguous cases.
\end{itemize}

\paragraph{Cost-Efficiency Ratios.}

Defining cost-efficiency as $\eta = \text{Success Rate} / \text{Relative Cost}$, DAA achieves:
\begin{itemize}
\item PlanCraft: $\eta_\text{DAA} = 0.70$ vs $\eta_\text{7B} = 0.70$ (\textbf{1.0$\times$}, same efficiency at 46\% lower cost)
\item WorkBench: $\eta_\text{DAA} = 0.291$ vs $\eta_\text{7B} = 0.100$ (\textbf{2.91$\times$})
\item FinanceBench: $\eta_\text{DAA} = 1.268$ vs $\eta_\text{7B} = 0.859$ (\textbf{1.48$\times$})
\item BrowseComp+: $\eta_\text{DAA} = 0.750$ vs $\eta_\text{7B} = 0.400$ (\textbf{1.88$\times$})
\end{itemize}

DAA consistently achieves efficiency improvements on 3 of 4 benchmarks, with PlanCraft demonstrating that cost-efficiency parity is achievable even when absolute accuracy differs from the baseline.

\subsection{Ablation Studies}
\label{sec:ablation}

We conduct ablation experiments to quantify the contribution of each DAA component.

\paragraph{Three-Way Classification vs Binary.}

We compare DAA's three-way classification (confident / hard\_unknown / soft\_uncertain) against simpler binary strategies on BrowseComp+:

\begin{table}[h]
\centering
\caption{Ablation of escalation strategies on BrowseComp+ (30 tasks). Three-way classification achieves optimal accuracy-cost balance.}
\label{tab:ablation_escalation}
\begin{tabular}{lccc}
\toprule
\textbf{Strategy} & \textbf{Accuracy} & \textbf{Cost Savings} & \textbf{Wasteful Calls} \\
\midrule
Always Escalate (v1)          & 40.0\% & 0\%    & 0 (but 100\% unnecessary) \\
Escalate All Uncertain (v3)   & 40.0\% & 10\%   & 14/15 (93\%) \\
Never Escalate (v4)           & 36.7\% & 60\%   & 0 (lost 1 answer) \\
\textbf{Selective Escalation (v5)} & \textbf{40.0\%} & \textbf{46.7\%} & \textbf{3/4 (75\%)} \\
\bottomrule
\end{tabular}
\end{table}

The three-way classification reduces wasteful escalation from 14 calls to 3 calls (\textbf{79\% reduction}) while recovering the answer lost by the ``never escalate'' variant. This demonstrates that distinguishing between \emph{model capacity limitations} and \emph{information limitations} is essential for cost-effective orchestration.

\paragraph{Model Tier Selection.}

On PlanCraft (proper protocol), we compare routing strategies:

\begin{table}[h]
\centering
\caption{Impact of model tier selection strategy on PlanCraft (50 tasks, official protocol). Phase-aware three-tier selection (DAA) significantly outperforms uniform routing to a single cheaper model.}
\label{tab:ablation_model_pair}
\begin{tabular}{lccc}
\toprule
\textbf{Strategy} & \textbf{Success Rate} & \textbf{Cost} & \textbf{Notes} \\
\midrule
3B only (Cascade)       & 26.0\%  & 34\% & 3B fails on complex reasoning \\
3B + 7B (FrugalGPT)     & 28.0\%  & 44\% & Wastes 7B on wrong steps \\
3B + minor 7B (RouteLLM) & 32.0\% & 32\% & Under-escalates to 7B \\
\textbf{1.5B/3B/7B (DAA)} & \textbf{38.0\%} & 54\% & Phase-aware three-tier \\
\bottomrule
\end{tabular}
\end{table}

This reveals that \emph{how} models are allocated across task phases matters as much as which models are available. DAA's phase-aware strategy (1.5B for search, 3B for smelting, 7B for reasoning) outperforms all other routing approaches, including those with access to the same model pool.

\paragraph{Escalation Threshold Sensitivity.}

On BrowseComp+, we analyze the sensitivity of performance to classification thresholds. The response length threshold (distinguishing hard\_unknown from soft\_uncertain) is the most critical parameter:

\begin{itemize}
\item \textbf{Too aggressive} (low threshold): Over-escalation, high cost, marginal accuracy gain
\item \textbf{Too conservative} (high threshold): Under-escalation, missed recoverable answers
\item \textbf{Optimal}: Response length $< 100$ words with $\geq 4$ uncertainty markers classifies as hard\_unknown
\end{itemize}

The selected thresholds generalize across benchmarks without per-benchmark tuning, though domain-specific tuning could yield additional 5--10\% cost savings.

\subsection{Scaling Analysis}

\paragraph{Impact of Task Complexity.}

The proper PlanCraft evaluation reveals a clear complexity-dependent performance profile (Table~\ref{tab:plancraft_complexity}). DAA \emph{exceeds} the 7B Baseline on complexity-1 tasks (69.6\% vs 60.9\%) but drops sharply on complexity-2/3 tasks. This indicates that DAA's phase-aware model selection is most effective when tasks can be decomposed into phases of varying difficulty---a common pattern in real-world agentic workflows.

\paragraph{Best-of-Both-Models Analysis.}

DAA succeeds on 5 tasks where the 7B-only baseline fails, demonstrating model diversity benefits:

\begin{table}[h]
\centering
\caption{Best-of-Both-Models effect on PlanCraft. DAA succeeds on 5 tasks where the 7B-only baseline fails, through beneficial diversity in action generation across model tiers.}
\label{tab:plancraft_complementary}
\begin{tabular}{llcl}
\toprule
\textbf{Task ID} & \textbf{Target Item} & \textbf{Complexity} & \textbf{DAA Model Mix} \\
\midrule
VAL0057  & birch\_button              & 1 & 1.5B:1, 3B:6, 7B:3 \\
VAL0095  & black\_glazed\_terracotta  & 1 & 1.5B:1, 3B:1, 7B:3 \\
VAL0163  & dried\_kelp                & 1 & 1.5B:1, 3B:1, 7B:1 \\
VAL0198  & light\_gray\_dye           & 2 & 1.5B:1, 3B:4, 7B:1 \\
VAL0229  & cooked\_salmon             & 1 & 1.5B:1, 7B:1       \\
\bottomrule
\end{tabular}
\end{table}

These cases reveal that 3B occasionally produces correct actions for tasks where 7B gets stuck in repetitive loops (e.g., birch\_button: 7B uses 21 steps and fails, while DAA solves it in 10). The model switching introduces \emph{diversity} in action generation that breaks single-model failure modes.

\subsection{Key Insights}

Our experimental evaluation reveals several important findings for practical LLM orchestration:

\textbf{1. Evidence-Aware Selective Escalation is the key innovation.}
The three-way response classification (confident / hard\_unknown / soft\_uncertain) achieves 79\% reduction in wasteful escalation compared to naive strategies, enabling cost savings without accuracy loss.

\textbf{2. Phase-aware model selection outperforms uniform routing.}
On PlanCraft, DAA's three-tier strategy (1.5B/3B/7B) achieves 38.0\% vs Cascade's 26.0\% using 3B-only, demonstrating that \emph{how} models are allocated across task phases matters as much as which models are used.

\textbf{3. Smaller models can exceed larger ones on specific task types.}
On PlanCraft complexity-1 tasks, DAA achieves 69.6\% vs the 7B Baseline's 60.9\%. On FinanceBench Medium tasks, the 3B model outperforms 7B (0.833 vs 0.793). These non-monotonic relationships validate DAA's adaptive model selection.

\textbf{4. Best-of-Both-Models effect through model diversity.}
PlanCraft demonstrates that DAA succeeds on 5 tasks where the 7B-only baseline fails, through beneficial diversity in action generation. Model switching breaks single-model failure modes (e.g., repetitive loops).

\textbf{5. Cost savings scale with task decomposability.}
Benchmarks with clearly separable difficulty levels or phases (WorkBench: 65.7\% savings, PlanCraft: 46\% savings) benefit most from dynamic routing. Complex monolithic tasks where all steps require sustained large-model reasoning show reduced routing benefits.

\subsection{Comparison with Existing Model Routing Methods}
\label{sec:sota_comparison}

We compare DAA against three representative model routing baselines, all evaluated on the same four benchmarks using identical model infrastructure:

\begin{itemize}
\item \textbf{FrugalGPT}~\cite{chen2023frugalgpt}: Static prompt-complexity-based routing. Computes a difficulty score from query features (length, vocabulary richness, domain keywords) and routes queries above a threshold to the expensive model. Does not observe model responses.
\item \textbf{RouteLLM}: Feature-based dynamic routing. Extends FrugalGPT with richer query analysis including multi-question detection and evidence quality assessment, simulating a trained classifier for difficulty prediction.
\item \textbf{Cascade}: Confidence-threshold sequential routing. Always tries the cheap model first and escalates to the expensive model if response confidence falls below a threshold. Uses binary escalation (escalate or keep) without three-way classification.
\end{itemize}

\begin{table}[t]
\centering
\caption{SOTA comparison: DAA vs existing model routing methods across all benchmarks. DAA achieves the best Pareto optimality (highest accuracy at lowest cost). Cost is relative to the large-model baseline (100\%). \textbf{Bold} indicates best per benchmark.}
\label{tab:sota_comparison}
\begin{tabular}{llccc}
\toprule
\textbf{Benchmark} & \textbf{Method} & \textbf{Accuracy} & \textbf{Cost (\%)} & \textbf{vs Baseline} \\
\midrule
\multirow{4}{*}{PlanCraft$^\dagger$}
 & FrugalGPT  & 28.0\% & 44\%  & $-$42.0\% \\
 & RouteLLM   & 32.0\% & 32\%  & $-$38.0\% \\
 & Cascade    & 26.0\% & 34\%  & $-$44.0\% \\
 & \textbf{DAA} & \textbf{38.0\%} & \textbf{54\%} & \textbf{$-$32.0\%} \\
\midrule
\multirow{4}{*}{BrowseComp+}
 & FrugalGPT  & 36.7\% & 40\%  & $-$3.3\% \\
 & RouteLLM   & 36.7\% & 86\%  & $-$3.3\% \\
 & Cascade    & 36.7\% & 43\%  & $-$3.3\% \\
 & \textbf{DAA} & \textbf{40.0\%} & \textbf{53\%} & \textbf{$\pm$0.0\%} \\
\midrule
\multirow{4}{*}{FinanceBench}
 & FrugalGPT  & 91.8\% & 60\%  & $\pm$0.0\% \\
 & RouteLLM   & \textbf{92.9\%} & 73\% & +1.1\% \\
 & Cascade    & 91.6\% & \textbf{40\%}  & $-$0.2\% \\
 & DAA        & 91.6\% & \textbf{40\%}  & $-$0.2\% \\
\midrule
\multirow{4}{*}{WorkBench}
 & FrugalGPT  & 16.7\% & 40\%  & $\pm$0.0\% \\
 & RouteLLM   & 16.7\% & 40\%  & $\pm$0.0\% \\
 & Cascade    & 16.7\% & 40\%  & $\pm$0.0\% \\
 & DAA        & 16.7\% & 40\%  & $\pm$0.0\% \\
\midrule
\multicolumn{2}{l}{\textbf{Average}} & & & \\
 & FrugalGPT  & 52.3\% & 59.5\% & eff: 0.879 \\
 & RouteLLM   & 54.1\% & 76.8\% & eff: 0.704 \\
 & Cascade    & 53.7\% & 57.8\% & eff: 0.929 \\
 & \textbf{DAA} & \textbf{54.6\%} & \textbf{56.3\%} & \textbf{eff: 0.969} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{DAA Dominates the Pareto Frontier.}

Table~\ref{tab:sota_comparison} and Figure~\ref{fig:pareto} show that DAA achieves the highest cost-efficiency ratio (0.969) across all methods. The key differentiators emerge on benchmarks where response-aware decisions matter:

\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{figures/paper_pareto_frontier.png}
\caption{Pareto frontier of cost vs accuracy across benchmarks. Each method's average position is shown as a large marker; individual benchmark points as smaller semi-transparent markers. DAA occupies the Pareto-optimal position (highest accuracy at lowest cost). The ideal region (top-left) is shaded.}
\label{fig:pareto}
\end{figure}

\begin{itemize}
\item \textbf{PlanCraft} (proper protocol): DAA achieves 38.0\%---the highest among all routing methods (vs FrugalGPT 28.0\%, RouteLLM 32.0\%, Cascade 26.0\%). While the full 7B Baseline reaches 70.0\%, DAA's phase-aware three-tier strategy (1.5B for search, 3B for smelting, 7B for reasoning) provides the most effective cost-accuracy trade-off among routing approaches.
\item \textbf{BrowseComp+}: Only DAA maintains baseline accuracy (40.0\%). All other methods degrade to 36.7\% because they either over-escalate (wasting cost) or under-escalate (losing recoverable answers). DAA's three-way classification uniquely identifies the 4 tasks where escalation helps.
\item \textbf{FinanceBench}: DAA matches Cascade at the lowest cost (40\%). RouteLLM achieves marginally higher accuracy (92.9\% vs 91.6\%) but at 83\% higher cost---an unfavorable trade-off.
\item \textbf{WorkBench}: All methods converge to identical performance (16.7\%, 40\% cost) because the task is uniformly solvable by 3B.
\end{itemize}

\paragraph{Why DAA Outperforms Other Routing Methods on PlanCraft.}

The critical distinction is DAA's \emph{phase-aware} model allocation. Cascade routes all calls to 3B and never escalates (0\% 7B usage), missing all tasks requiring complex reasoning. FrugalGPT escalates 31.1\% of calls to 7B but wastes many on steps where 3B sufficed. DAA uniquely assigns each model to the task phase matching its capabilities: 1.5B for search (14.6\%), 3B for simple actions (41.7\%), and 7B for recipe interpretation and error recovery (43.7\%).

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/paper_plancraft_deep.png}
\caption{PlanCraft deep dive (proper multi-step protocol). \textbf{Left}: Cost-accuracy scatter showing DAA achieves the highest success rate among all routing methods (38.0\%) at 54\% of baseline cost. \textbf{Right}: Model usage breakdown. DAA's three-tier distribution (1.5B/3B/7B) stands in contrast to Cascade (100\% 3B) and RouteLLM (98\% 3B), demonstrating the advantage of phase-aware model allocation.}
\label{fig:plancraft_deep}
\end{figure}

\paragraph{Accuracy Across Benchmarks.}

Figure~\ref{fig:accuracy_bars} compares accuracy across all benchmarks. On BrowseComp+, DAA is the only routing method to maintain baseline accuracy (40.0\%), while all three competitors degrade to 36.7\%. On PlanCraft, DAA achieves the highest routing-method accuracy (38.0\%), outperforming FrugalGPT, RouteLLM, and Cascade by 6--12 percentage points.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/paper_accuracy_bars.png}
\caption{Task success rate comparison across all four benchmarks. DAA (rightmost blue bar, highlighted) matches or exceeds the baseline (dashed line) on every benchmark. All three competing methods fail to match the baseline on BrowseComp+ (36.7\% vs 40.0\%), demonstrating the importance of evidence-aware escalation.}
\label{fig:accuracy_bars}
\end{figure}

