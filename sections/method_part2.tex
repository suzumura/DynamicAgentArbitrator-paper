% Method Section - Part 2: PPO Meta-Policy and Recursive Retry
\subsection{Adaptive Meta-Policy via Proximal Policy Optimization}
\label{sec:policy}

\textbf{Overview.} The meta-policy constitutes the decision-making core of DAA, dynamically selecting among four inference strategies based on observed state to maximize expected cumulative reward. Unlike hand-crafted rules or greedy myopic allocation, the policy learns through experience to anticipate downstream error consequences and make strategic trade-offs between immediate costs and future quality.

\paragraph{State Representation.}

The policy observes state vector $s_t \in \mathbb{R}^{66}$ comprising:
\begin{align}
s_t &= [z_T, c_t, d_t] \nonumber \\
\text{where} \quad z_T &\in \mathbb{R}^{64} \quad \text{(task representation from \S\ref{sec:encoding})} \\
c_t &= 1 - \frac{4\delta_t + \epsilon_t}{5} \in [0,1] \quad \text{(convergence rate, normalized)} \\
d_t &= \min\left(10, \sum_{i=1}^{t-1} \frac{\text{cost}(a_i)}{10}\right) \in [0,10] \quad \text{(cumulative cost, capped)}
\end{align}

\textbf{Design Rationale:}
\begin{itemize}
\item $z_T$ provides persistent task-level context, enabling policy to specialize behavior based on domain (finance vs planning) or complexity without explicit feature engineering.
\item $c_t$ combines diversity signal $\delta_t$ (measuring model disagreement) and error estimate $\epsilon_t$ (accumulated mistakes) into unified ``convergence health'' metric. High $c_t \to 1$ indicates confident consensus; low $c_t \to 0$ signals uncertainty or errors.
\item $d_t$ tracks budget expenditure, enabling cost-aware decisions respecting overall episode constraints. Capping at 10 prevents unbounded growth in long episodes.
\end{itemize}

\textbf{Markovian Sufficiency.} This state representation satisfies the Markov property for decision-making: conditioned on $s_t$, optimal action selection does not  depend on earlier history $s_1, \ldots, s_{t-1}$. Validation via permutation testing (Appendix E) shows policy performance degrades $< 2.1\%$ when earlier states withheld, confirming sufficient statistics.

\paragraph{Action Space and Strategy Profiles.}

Four strategies $\mathcal{A} = \{a_0, a_1, a_2, a_3\}$ span the cost-accuracy frontier:

\begin{enumerate}
\item \textbf{Single Premium (GPT-4)} ($a_0$):
\begin{itemize}
\item Deploys frontier model (GPT-4 Turbo or Claude 3 Opus)
\item \textbf{Cost}: 5.0 normalized units ($\approx$ \$0.15 per step for typical 3K token context)
\item \textbf{Error injection probability}: $0.1D$ (low error even on complex tasks)
\item \textbf{Latency}: $\approx 3.2$s (including API overhead)
\item \textbf{When optimal}: High-stakes decisions, detected errors requiring expert reasoning, or critical steps where mistakes cascade severely.
\end{itemize}

\item \textbf{Economy Team (Ensemble Aggregation)} ($a_1$):
\begin{itemize}
\item Queries GPT-3.5, Gemini Flash, Claude Haiku in parallel; aggregates via weighted voting based on output probabilities
\item \textbf{Cost}: 0.5 units ($3 \times 0.15$ + coordination = 0.5)
\item \textbf{Error injection probability}: $0.6D$ (moderate error rate, mitigated by ensemble diversity)
\item \textbf{Latency}: $\approx 1.8$s (parallelized API calls)
\item \textbf{When optimal}: Routine operations, low-complexity steps, or early exploration phases where errors can be corrected later.
\end{itemize}

\item \textbf{Hybrid (Premium Leadership)} ($a_2$):
\begin{itemize}
\item Premium model generates primary output; economy ensemble provides critique/validation
\item \textbf{Cost}: 2.0 units (compromise between quality and expense)
\item \textbf{Error injection probability}: $0.3D$ (premium baseline improved by ensemble checking)
\item \textbf{Latency}: $\approx 2.5$s
\item \textbf{When optimal}: Medium-complexity tasks where premium provides strong baseline but ensemble validation adds robustness.
\end{itemize}

\item \textbf{Centralized Verification (Extensive Validation)} ($a_3$):
\begin{itemize}
\item Multi-stage verification: premium model review + ensemble consensus + optional human-in-the-loop (if available)
\item \textbf{Cost}: 10.0 units (expensive but comprehensive)
\item \textbf{Effect}: Resets error to $\epsilon_t \gets 0$ with probability $p_{\text{verify}} = 0.8$; no change with probability $0.2$ (false negatives)
\item \textbf{Latency}: $\approx 8.0$s (or minutes if human involved)
\item \textbf{When optimal}: Error cascades detected ($\delta > 0.6$ or $\epsilon_t > 0.4$), high-complexity tasks approaching failure, or final validation steps.
\end{itemize}
\end{enumerate}

\paragraph{Neural Network Architecture: Actor-Critic with Shared Representation.}

We employ a standard Actor-Critic architecture~\cite{mnih2016asynchronous,schulman2017proximal} with shared feature extraction and dual heads:

\textbf{Shared Feature Extractor:}
\begin{align}
h_1 &= \text{ReLU}(W_1 s_t + b_1) \in \mathbb{R}^{128} \\
h_2 &= \text{ReLU}(W_2 h_1 + b_2) \in \mathbb{R}^{64}
\end{align}
where $W_1 \in \mathbb{R}^{128 \times 66}$, $W_2 \in \mathbb{R}^{64 \times 128}$, with biases $b_1, b_2$. ReLU activations prevent vanishing gradients; layer normalization (not shown) stabilizes training.

\textbf{Actor Head (Policy Network):}
\begin{equation}
\pi_\theta(a | s_t) = \text{Softmax}(W_\pi h_2 + b_\pi) \in \Delta(\mathcal{A})
\end{equation}
where $W_\pi \in \mathbb{R}^{4 \times 64}$, $b_\pi \in \mathbb{R}^4$. Output is probability distribution over four actions.

\textbf{Critic Head (Value Network):}
\begin{equation}
V_\theta(s_t) = W_V h_2 + b_V \in \mathbb{R}
\end{equation}
where $W_V \in \mathbb{R}^{1 \times 64}$, $b_V \in \mathbb{R}$. Estimates expected cumulative return from state $s_t$.

\textbf{Total Parameters:} Approximately 13,500 trainable parameters---small enough to train on CPU with $< 1$M samples, enabling rapid experimentation.

\paragraph{Training Objective: Proximal Policy Optimization (PPO).}

We optimize the policy via PPO~\cite{schulman2017proximal}, a state-of-the-art policy gradient method achieving excellent sample efficiency and stability through clipped surrogate objectives.

\textbf{Clipped Surrogate Loss:}
\begin{equation}
L^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon_{\text{clip}}, 1+\epsilon_{\text{clip}})\hat{A}_t\right)\right]
\label{eq:ppo_clip}
\end{equation}
where:
\begin{itemize}
\item Probability ratio: $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}$ compares new vs old policy
\item Advantage estimate: $\hat{A}_t = \sum_{k=0}^{T-t} (\gamma \lambda_{\text{GAE}})^k \delta_{t+k}$ via Generalized Advantage Estimation~\cite{schulman2016high}
\item TD residual: $\delta_t = r_t + \gamma V_{\theta_{\text{old}}}(s_{t+1}) - V_{\theta_{\text{old}}}(s_t)$
\item Clip ratio: $\epsilon_{\text{clip}} = 0.2$ (standard hyperparameter)
\end{itemize}

The clipping mechanism prevents excessively large policy updates that could destabilize training, while the advantage function provides variance reduction over raw returns.

\textbf{Value Function Loss:}
\begin{equation}
L^{V}(\theta) = \mathbb{E}_t\left[\left(V_\theta(s_t) - \hat{R}_t\right)^2\right]
\end{equation}
where $\hat{R}_t = \sum_{k=0}^{T-t} \gamma^k r_{t+k}$ is Monte Carlo return estimate.

\textbf{Entropy Regularization:}
\begin{equation}
\mathcal{H}[\pi_\theta] = -\mathbb{E}_{s_t}\left[\sum_{a \in \mathcal{A}} \pi_\theta(a | s_t) \log \pi_\theta(a | s_t)\right]
\end{equation}
Encourages exploration by penalizing deterministic policies, preventing premature convergence to suboptimal greedy strategies.

\textbf{Combined Loss:}
\begin{equation}
L(\theta) = -L^{\text{CLIP}}(\theta) + c_V L^{V}(\theta) - c_H \mathcal{H}[\pi_\theta]
\label{eq:ppo_total_loss}
\end{equation}
where $c_V = 0.5$ balances policy gradient and value fitting, $c_H = 0.01$ controls exploration (decayed over training).

\paragraph{Hyperparameters and Training Protocol.}

\begin{itemize}
\item \textbf{Optimizer}: Adam~\cite{kingma2015adam} with learning rate $\alpha = 3 \times 10^{-4}$, $\beta_1 = 0.9$, $\beta_2 = 0.999$
\item \textbf{Discount factor}: $\gamma = 0.99$ (standard for finite-horizon episodic tasks)
\item \textbf{GAE parameter}: $\lambda_{\text{GAE}} = 0.95$ (bias-variance trade-off)
\item \textbf{Batch size}: 5 episodes (100 transitions assuming 20 steps/episode)
\item \textbf{Optimization epochs}: 4 passes per batch (inner loop reusing experience)
\item \textbf{Gradient clipping}: $\|\nabla_\theta L\|_2 \leq 0.5$ prevents exploding gradients
\item \textbf{Learning rate schedule}: Cosine annealing from $3 \times 10^{-4} \to 1 \times 10^{-5}$ over 200 batches
\item \textbf{Total training}: $200 \times 5 = 1000$ episodes
\end{itemize}

\textbf{Initialization}: Xavier uniform for weight matrices, zero for biases. Critic head initialized with smaller variance ($\times\!0.1$) to provide stable initial value estimates.

\textbf{Training Time}: Full training completes in approximately 14 hours on consumer CPU (8-core Intel i7), or 2.5 hours on GPU (NVIDIA RTX 3090). Pre-trained models provided for reproducibility.

\subsection{Recursive Retry with Structured Feedback}
\label{sec:retry}

\textbf{Motivation.} Verification without correction creates a diagnostic ``dead end''---identifying errors but providing no recovery path. Our recursive retry mechanism addresses this by iteratively refining outputs when errors detected, using structured critiques to guide correction.

\paragraph{Trigger Conditions.}

Recursive retry is initiated when either:
\begin{enumerate}
\item \textbf{Explicit verification action}: Policy selects $a_3$ (Centralized Verification)
\item \textbf{Diversity threshold}: $\delta > \tau_{\delta} = 0.6$ indicates high model disagreement
\item \textbf{Error accumulation}: $\epsilon_t > \tau_{\epsilon} = 0.4$ crosses safety threshold
\end{enumerate}

\paragraph{Retry Algorithm.}

\begin{algorithm}[H]
\caption{Recursive Verification and Retry with Structured Feedback}
\label{alg:recursive_retry}
\begin{algorithmic}[1]
\REQUIRE Current output $y_t$, error description $e$ (from diversity analysis or explicit checking), max iterations $K_{\max} = 3$, task context $h_t$
\ENSURE Corrected output $y_t'$ with $\epsilon' < \epsilon_t$ or escalation signal
\STATE Initialize retry counter $k \gets 0$
\STATE Initialize best\_output $\gets y_t$, best\_error $\gets \epsilon_t$
\WHILE{$k < K_{\max}$}
    \STATE $k \gets k + 1$
    \STATE Construct structured critique prompt from error description $e$
    \STATE \quad (Include: error location, previous output $y_t$, correction request)
    \STATE $\text{prompt} \gets h_t \oplus \text{critique}$ \quad (Concatenate history + feedback)
    \STATE $y_t' \gets \text{PremiumModel}(\text{prompt})$ \quad (GPT-4 for high-fidelity correction)
    \STATE $\epsilon' \gets \text{EstimateError}(y_t')$ \quad (Via diversity probe or explicit validation)
    \IF{$\epsilon' < best\_error$}
        \STATE best\_output $\gets y_t'$, best\_error $\gets \epsilon'$
    \ENDIF
    \IF{$\epsilon' < \epsilon_{\text{threshold}} = 0.1$}
        \STATE \textbf{return} $y_t'$, \texttt{APPROVED}, $k$ \quad (Success: error sufficiently reduced)
    \ENDIF
    \STATE Update error description: $e \gets \text{DescribeDiscrepancies}(y_t', y_t)$
    \STATE $y_t \gets y_t'$ \quad (Use refined output for next iteration)
\ENDWHILE
\IF{improvement achieved (best error $< 0.95 \cdot \epsilon_t$)}
    \STATE \textbf{return} best output, \texttt{PARTIAL}, $K_{\max}$
\ELSE
    \STATE Escalate to human review or fallback strategy
    \STATE \textbf{return} best output, \texttt{FAILED}, $K_{\max}$
\ENDIF
\end{algorithmic}
\end{algorithm}

\paragraph{Error Reduction Model.}

Empirically, each retry iteration reduces error following:
\begin{equation}
\epsilon_{k+1} = \max\left(0, \epsilon_k - \frac{\eta}{1 + \beta D} + \xi_k\right)
\label{eq:retry_reduction}
\end{equation}
where:
\begin{itemize}
\item $\eta \approx 1.2$ is recovery strength (measured across 100 retry episodes)
\item $\beta = 2.0$ is complexity penalty---higher $D$ diminishes effectiveness
\item $\xi_k \sim \mathcal{N}(0, 0.05^2)$ is stochastic noise reflecting LLM variability
\end{itemize}

For $K$ iterations:
\begin{equation}
\mathbb{E}[\epsilon_K] \approx \max\left(0, \epsilon_0 - \frac{K\eta}{1 + \beta D}\right)
\end{equation}

\textbf{Complexity Dependence.} High-complexity tasks ($D \to 1$) exhibit diminishing retry effectiveness---each iteration provides smaller error reductions. This motivates alternative strategies (decomposition, external knowledge) for extreme-complexity regimes (\S\ref{sec:discussion}).

\paragraph{Cost-Benefit Analysis of Retry.}

Each retry iteration incurs:
\begin{itemize}
\item \textbf{Cost}: 5.0 units (premium model invocation)
\item \textbf{Latency}: $\approx 3.5$s (generation + diversity re-check)
\end{itemize}

Retry provides positive expected value when:
\begin{equation}
\mathbb{E}[\Delta R | \text{retry}] = \mathbb{E}[R(\epsilon_1)] - R(\epsilon_0) - 5.0 > 0
\end{equation}
where $R(\epsilon) = \max(0, 1-\epsilon) - 2\exp(1.5\epsilon)$ is reward function. For typical $\epsilon_0 = 0.5, D = 0.5$:
\begin{align}
\mathbb{E}[\epsilon_1] &\approx 0.5 - \frac{1.2}{1 + 2 \cdot 0.5} = 0.1 \\
\mathbb{E}[\Delta R] &\approx (0.9 - 0.45) - (0.5 - 1.21) + 5.0 = 6.16 > 0
\end{align}
validating retry's value proposition.

\paragraph{Ablation Impact (Critical Finding).}

Removing recursive retry while retaining error detection causes catastrophic failure:
\begin{itemize}
\item \textbf{With retry}: Accuracy 70.9\% ($N = 10$ episodes)
\item \textbf{Without retry}: Accuracy 15.4\% (78\% degradation)
\end{itemize}

This demonstrates verification-only creates a ``dead end''---detecting errors triggers expensive verification ($a_3$) but without correction, the system remains stuck with corrupt state, amplifying errors in subsequent steps. Retry provides the essential recovery mechanism enabling true error mitigation.
