% Discussion and Conclusion Section
\section{Discussion}
\label{sec:discussion}

\subsection{Summary of Contributions}

This work introduces the Dynamic Agent Arbitrator (DAA), a framework for cost-efficient multi-agent LLM orchestration. Our key contributions are:

\begin{enumerate}
\item \textbf{Evidence-Aware Selective Escalation}: A three-way response classification (confident / hard\_unknown / soft\_uncertain) that reduces wasteful model escalation by 79\% while maintaining accuracy. This mechanism distinguishes between model capacity limitations and information limitations---a distinction critical for cost-effective deployment.

\item \textbf{Phase-Aware Multi-Tier Model Selection}: On PlanCraft, evaluated using the official multi-step interactive protocol~\cite{gautierdag2024plancraft}, DAA's three-tier strategy (1.5B for search, 3B for smelting, 7B for reasoning) achieves 38.0\% success rate---the highest among all routing methods---at 54\% of baseline cost.

\item \textbf{Best-of-Both-Models Effect}: Demonstration that multi-model routing provides robustness beyond any single model. DAA succeeds on 5 PlanCraft tasks where the 7B-only baseline fails, and achieves 69.6\% on complexity-1 tasks---surpassing the 7B Baseline's 60.9\%.

\item \textbf{Comprehensive Evaluation}: Validation across four diverse benchmarks with head-to-head comparison against FrugalGPT, RouteLLM, and Cascade. DAA achieves the highest accuracy among routing methods on every benchmark, with 32--66\% cost savings.

\item \textbf{Theoretical Foundations}: Formal analysis of error propagation bounds under intervention and PPO convergence guarantees, providing principled grounding for algorithm design.
\end{enumerate}

\subsection{Practical Implications}

\paragraph{Deployment Recommendations.}

Our experiments reveal several actionable guidelines for practitioners deploying multi-LLM systems:

\begin{itemize}
\item \textbf{Phase-aware model selection is the most impactful decision.} On PlanCraft, DAA's three-tier strategy (1.5B for search, 3B for smelting, 7B for reasoning) achieves 38.0\% vs Cascade's 26.0\% using 3B-only. Allocating models to appropriate task \emph{phases} matters as much as which models are available.

\item \textbf{Simple heuristics often suffice.} The Evidence-Aware Selective Escalation uses straightforward response analysis (length, confidence markers, uncertainty patterns) without requiring expensive neural controllers. In resource-constrained settings, well-calibrated heuristics provide excellent cost-efficiency.

\item \textbf{Smaller models can outperform larger ones on specific task types.} On PlanCraft complexity-1 tasks (smelting), DAA achieves 69.6\% vs the 7B Baseline's 60.9\%. On FinanceBench Medium tasks, the 3B model exceeds 7B performance (0.833 vs 0.793). These non-monotonic relationships validate the need for empirical model-task matching.

\item \textbf{Model diversity improves robustness.} DAA succeeds on 5 PlanCraft tasks where the 7B-only baseline fails, through beneficial diversity in action generation. Model switching breaks single-model failure modes (e.g., 7B loops on birch\_button for 21 steps).
\end{itemize}

\paragraph{Cost-Efficiency in Production.}

At production scale (millions of queries/day), DAA's 39\% average cost reduction translates to significant savings. For a service processing 10M queries/day at \$0.50/query (7B pricing), DAA would reduce costs by approximately \$1.95M/day while maintaining equivalent quality. The 1.9$\times$ efficiency improvement enables serving more users within fixed compute budgets.

\subsection{Limitations}

\paragraph{Statistical Power.}

Sample sizes range from 18 (FinanceBench) to 60 (WorkBench) tasks per benchmark. While trends are consistent across benchmarks, individual benchmark results have wide confidence intervals. For example, PlanCraft's 70.0\% (50 tasks) has 95\% CI approximately $[56\%, 82\%]$. Larger-scale evaluation would strengthen statistical claims.

\paragraph{Task Difficulty Distribution.}

Performance varies substantially with task difficulty. On PlanCraft val.small (mixed difficulty, 110 tasks), the early 3B+7B configuration showed 13.6\% accuracy vs 19.1\% baseline---a degradation attributable to insufficient model capacity for hard tasks. The solution (7B+14B pairing) requires prior knowledge of task difficulty distributions.

\paragraph{Model-Specific Behavior.}

Our results are obtained with the Qwen2.5 model family. Different model families (LLaMA, Mistral, GPT) may exhibit different knowledge boundaries and capacity scaling, potentially affecting optimal model pairing and escalation thresholds. Cross-family generalization requires further investigation.

\paragraph{Single-Turn Evaluation.}

Current evaluation uses single-turn (one-shot) inference. Many real-world applications involve multi-turn agentic workflows where DAA's model selection could be applied at each turn. Multi-turn evaluation with tool use would better reflect practical deployment scenarios.

\paragraph{Escalation Overhead.}

The try-cheap-first strategy incurs latency overhead when escalation occurs (two model calls instead of one). For latency-sensitive applications, this overhead may be unacceptable. A confidence-based routing approach (predict which model to use \emph{before} any inference) would eliminate this overhead at the cost of prediction accuracy.

\subsection{Future Work}

\paragraph{Learned Escalation Policies.}

While our heuristic-based escalation achieves strong results, a learned policy trained on larger datasets could capture more nuanced escalation decisions. Reinforcement learning with cost-aware reward functions could optimize the accuracy-cost trade-off end-to-end.

\paragraph{Confidence-Based Pre-Routing.}

Instead of try-cheap-first (which adds latency), a router model could predict the appropriate tier from the query alone, enabling single-call inference. This requires training a lightweight classifier on query-outcome pairs, trading some accuracy for latency reduction.

\paragraph{Multi-Turn Agent Orchestration.}

Extending DAA to multi-turn workflows where model selection decisions compound across conversation turns. The MDP formulation in \S\ref{sec:problem} naturally supports this extension, with state tracking across turns.

\paragraph{Cross-Family Model Pools.}

Current evaluation uses models from a single family (Qwen2.5). Heterogeneous pools combining models from different families (e.g., Qwen for coding, LLaMA for reasoning, Mistral for instruction following) could exploit complementary strengths more effectively.

\paragraph{Dynamic Threshold Adaptation.}

Current escalation thresholds are fixed across tasks within a benchmark. Online adaptation based on accumulated success/failure statistics could improve performance as the system processes more queries, implementing a form of meta-learning at deployment time.

\section{Conclusion}
\label{sec:conclusion}

We have presented the Dynamic Agent Arbitrator (DAA), a framework for cost-efficient multi-agent LLM orchestration that adaptively selects among heterogeneous model tiers based on estimated task characteristics. Through comprehensive evaluation on four real-world benchmarks---including the official multi-step interactive protocol for PlanCraft---and head-to-head comparison with existing model routing methods (FrugalGPT, RouteLLM, Cascade), we demonstrate that:

\begin{enumerate}
\item DAA achieves the \textbf{highest success rate among all routing methods} on every evaluated benchmark, outperforming competitors by 6--12 percentage points on PlanCraft.

\item DAA's \textbf{phase-aware three-tier model selection} (1.5B for search, 3B for smelting, 7B for reasoning) on PlanCraft achieves 38.0\% at 54\% of baseline cost, and achieves \textbf{69.6\% on complexity-1 tasks}---surpassing even the 7B-only Baseline (60.9\%).

\item The Evidence-Aware Selective Escalation mechanism reduces wasteful escalation by \textbf{79\%} on BrowseComp+, where DAA is the only routing method to maintain baseline accuracy (40.0\%).

\item The \textbf{Best-of-Both-Models effect} demonstrates that multi-model routing provides robustness through diversity: DAA succeeds on 5 PlanCraft tasks where the 7B-only baseline fails, by breaking single-model failure modes via model switching.

\item DAA reduces inference costs by \textbf{32--66\%} across benchmarks while maintaining the highest routing-method accuracy.
\end{enumerate}

These results demonstrate that evidence-aware, phase-aware orchestration of heterogeneous LLM pools is a practical and effective strategy for reducing inference costs in both single-turn and multi-step agentic settings. As LLM deployments scale to handle millions of daily queries, the cost savings and robustness improvements provided by approaches like DAA become increasingly critical for sustainable and accessible AI systems.
