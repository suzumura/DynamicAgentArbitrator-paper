% Abstract
\begin{abstract}
Multi-agent LLM systems achieve strong task performance through ensemble reasoning, but incur prohibitive inference costs due to uniform deployment of expensive frontier models regardless of task difficulty. We introduce the \textbf{Dynamic Agent Arbitrator (DAA)}, a meta-learning framework that adaptively selects among heterogeneous LLM tiers based on estimated task complexity, achieving cost-efficient multi-agent orchestration with provable error mitigation guarantees.

DAA integrates four synergistic components: (1) neural task encoding via Sentence Transformers and GRU networks that compress reasoning traces into latent representations capturing complexity; (2) diversity-based error detection via Jensen-Shannon Divergence across model outputs, providing training-free error signals (Pearson $r = 0.385$ with ground-truth errors); (3) a PPO-trained meta-policy selecting among inference strategies; and (4) \textbf{Evidence-Aware Selective Escalation}, a three-way response classification distinguishing confident answers, hard unknowns, and soft uncertainties to minimize wasteful model escalation.

We evaluate DAA on four real-world benchmarks using Qwen2.5 models (1.5B--14B) served on H100 GPUs. On PlanCraft, we adopt the \emph{official} multi-step interactive protocol~\cite{gautierdag2024plancraft}, with up to 30 environment interaction steps per episode. Across PlanCraft (multi-step planning), WorkBench (office automation), FinanceBench (financial QA), and BrowseComp-Plus (evidence extraction), DAA achieves the \textbf{highest success rate among all routing methods} on every benchmark while reducing inference costs by \textbf{32--66\%}. In head-to-head comparison against FrugalGPT, RouteLLM, and Cascade, DAA dominates on both accuracy and cost-efficiency. On PlanCraft, DAA's phase-aware three-tier model selection (1.5B for search, 3B for smelting, 7B for reasoning) achieves 38.0\% success---outperforming all competing routing methods by 6--12 percentage points at 54\% of baseline cost. DAA also demonstrates a best-of-both-models effect, succeeding on 5 tasks where the 7B-only baseline fails, through model diversity in action generation. On complexity-1 tasks, DAA achieves 69.6\%, \emph{surpassing} even the 7B Baseline (60.9\%). On BrowseComp+, DAA is the only method to match baseline accuracy (40.0\%), while the selective escalation mechanism reduces wasteful expensive-model invocations by 79\%. Theoretical analysis establishes bounded error propagation under intervention and monotonic policy improvement guarantees.
\end{abstract}
